{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "Now that we have seen how a neural network is represented, we can go on to see how exactly it works. Since there are many layers having many neurons, there exists a complex set of weights to get an output from some input variables. Each weight in this network can be changed and hence there are countless configurations a neural network can have.\n",
    "\n",
    "A **trained** neural network has weights configuration which accurately predicts correct outputs for some input data. And that is what we ultimately hope to achieve. We will now go through how exactly a neural network trains itself to get this desirable weight configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Backpropagation is the name of the algorithm a neural network uses to train itself. This revolutionary algorithm is a mixture of the **chain rule in derivation** and **gradient descent**, the common optimization algorithm used in linear and logistic regression.\n",
    "\n",
    "To understand how backpropagation works, first we have to understand the relationship between the output and the weights in between. It is clear that every weight in the neural network will affect the output in some way due to the way the neural network is connected. Due to this fact, we can say that if I **change** a particular weight, the output will **change** in some way. We can also find the exact mathematical equation defining the relationship between each weight and the output.\n",
    "\n",
    "<center><img src=\"img/backprop_1.png\" alt=\"Dataset with three input features\" width=\"500\" height=\"233\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 15.</b> A dataset with three input features (X1, X2, and X3) </i></p>\n",
    "\n",
    "Let’s say we are given a dataset with three input features, ($x_1$, $x_2$ and $x_3$) and we need to find the relationship between the input features and the output using a neural network. We will now see what exactly the neural network does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimize the error\n",
    "\n",
    "<center><img src=\"img/backprop_2.png\" alt=\"error between the output and expected value\" width=\"580\" height=\"301\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 16.</b> Feed the neural network and measure the error between the output and expected value</i></p>\n",
    "\n",
    "First step is to feed in the data and getting the output from the neural network. We shall call the output, `Y_pred`. Next we should compare the predicted value with the actual value. This value will be called the error. It is essentially how bad or how far off the model is from predicting the values correctly. Our goal is to now **minimize the error**.\n",
    "\n",
    "Since `Y_pred` is a function of all the weights in the model and `Error` is a function of `Y_pred`, we can say that the `Error` will also depend on the weights. This means that we need to adjust our weights in such a way that the error is minimized. We do that using **partial derivatives**.\n",
    "\n",
    "Let’s take a simple example of an equation, $y = w_1*x_1 + w_2*x_2$. If we find the partial derivative of $w_1$ or $w_2$ with respect to $y$, we can find out how $w_1$ or $w_2$ can affect $y$.\n",
    "\n",
    "If the partial derivative of $w_1$ with respect to $y$ is **POSITIVE**, that means **DECREASING** the weight will **DECREASE** $y$.\n",
    "\n",
    "If the partial derivative of $w_1$ with respect to $y$ is **NEGATIVE**, that means **INCREASING** the weight will **DECREASE** $y$.\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_3.png\" alt=\"Derivatives measure the slope\" width=\"500\" height=\"294\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 17.</b> Derivatives measure the slope and in which direction to descent in order to change the output value</i></p>\n",
    "\n",
    "These two rules are all we need to optimize the weights in the neural network. We need to find the **partial derivative of the error** with respect to every weight. If that partial derivative is **positive**, then we **decrease** the value of that weight so that error gets decreased. If that partial derivative is **negative**, then we **increase** that weight so that the error gets decreased. This is the basic underlying concept of how weights are updated after we calculate the error.\n",
    "\n",
    "Since the last layer is the closest to the error, we will first derive the last layer with respect to the error and update those weights. Then we will move to the second last layer to do the same and so on and so forth. We repeat this process till we reach the first layer and all the weights are updated. This entire process is called **backpropagation**.\n",
    "\n",
    "<center><img src=\"img/backprop_4.gif\" alt=\"Backpropagation\" width=\"640\" height=\"480\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 18.</b> Backpropagation - Derive and update</i></p>\n",
    "\n",
    "We perform backpropagation for a single row of data and update the weights. We then repeat for all the data available in the training data set, this entire cycle is called **one epoch**. Usually neural networks can take several epochs to train and it is up to us to decide how many epochs it will train for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical explanation\n",
    "\n",
    "Let's say we have a neural network with 4 layers, each of them 1 neuron.\n",
    "\n",
    "Let's name the output of each layer $a^{(L)}$, $a^{(L-1)}$, $a^{(L-2)}$, $a^{(L-3)}$\n",
    "\n",
    "<center><img src=\"img/backprop_10.png\" alt=\"Backpropagation Simple NN\" width=\"500\" height=\"154\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 19.</b> Simple network with 4 neurons</i></p>\n",
    "\n",
    "The desired output $y$ is $1$.\n",
    "\n",
    "So the cost of this simple network is:\n",
    "$$ C_0 = (a^{(L)} - y)^2$$\n",
    "\n",
    "where $C_0$ is the cost for single training example.\n",
    "\n",
    "<center><img src=\"img/backprop_11.png\" alt=\"Backpropagation Cost calculation\" width=\"500\" height=\"251\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 20.</b> Cost calculation</i></p>\n",
    "\n",
    "The last ouput $a^{(L)}$ is calculated by next formula:\n",
    "$$a^{(L)} = \\sigma(w^{(L)} a^{(L-1)} + b^{(L)}) $$\n",
    "\n",
    "where $\\sigma$ is the activation function (e.g. *sigmoid* or *ReLU*)\n",
    "\n",
    "For the sake of clarity let's name the input for the activation function $z$ :\n",
    "$$z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} $$\n",
    "\n",
    "$$a^{(L)} = \\sigma(z^{(L)}) $$\n",
    "\n",
    "So, now we need to identify the importance of each weight $w$ to the cost. Because by modifying the weights we can modify (minimize) the cost. To do this we need to calculate the derivative of $w^{(L)}$ w.r.t $C_0$.\n",
    "\n",
    "$$ \\frac{dC_0}{dw^{(L)}}$$\n",
    "\n",
    "Using the **Chain Rule** we directly come up with:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{dw^{(L)}} = \\frac{dz^{(L)}}{dw^{(L)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "This would give us the sensitivity of $C$ to small changes in $w^{(L)}$.\n",
    "\n",
    "Now, let's calculate all the three derivatives, starting from the last one:\n",
    "\n",
    "$$ \\frac{dC_0}{da^{(L)}} = \\frac{d(a^{(L)} - y)^2}{da^{(L)}} = 2(a^{(L)} - y) $$\n",
    "\n",
    "The derivative of $a^{(L)}$ w.r.t $z^{(L)}$ is simply the derivative of the activation function:\n",
    "\n",
    "$$ \\frac{da^{(L)}}{dz^{(L)}} =  \\frac{d\\sigma(z^{(L)})}{dz^{(L)}} =   \\sigma ' (z^{(L)}) $$\n",
    "\n",
    "And the derivative of $z^{(L)}$ w.r.t $w^{(L)}$ comes up to be:\n",
    "\n",
    "$$ \\frac{dz^{(L)}}{dw^{(L)}} = \\frac{d(w^{(L)} a^{(L-1)} + b^{(L)})}{dw^{(L)}} = a^{(L-1)} $$\n",
    "\n",
    "So, finally the derivative of $C_0$ w.r.t to $w^{(L)}$ is:\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{dw^{(L)}} = a^{(L-1)} \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have not a single, but many training examples, the derivative of the full cost function for all examples could simply be calculated as the average of all training examples:\n",
    "\n",
    "$$ \\frac{dC}{dw^{(L)}} = \\frac{1}{m} \\sum_{i=0}^{m-1}\\frac{dC_i}{dw^{(L)}}$$\n",
    "\n",
    "The sensitivity to the bias is almost identical:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{db^{(L)}} = \\frac{dz^{(L)}}{db^{(L)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "and evaluated to:\n",
    "\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{db^{(L)}} = 1 \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to see how sensitive is the cost function the output $a^{(L-1)}$ of the previous layer:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{da^{(L-1)}} = \\frac{dz^{(L)}}{da^{(L-1)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "$$  \\frac{dz^{(L)}}{da^{(L-1)}} = \\frac{d(w^{(L)} a^{(L-1)} + b^{(L)})}{da^{(L-1)}} = w^{(L)}$$\n",
    "\n",
    "So, this is evaluated to:\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{da^{(L-1)}} = w^{(L)} \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$\n",
    "\n",
    "We can just keep iterating this same **chain rule** idea backwards to see how sensitive the cost function is to previous weights and biases.\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_12.png\" alt=\"Backpropagation derivatives\" width=\"295\" height=\"538\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 21.</b> Backpropagation - Derive cost sensitivity to previous weights and biases</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement a simple network with 3 layers:\n",
    " * Input layer with 2 nodes\n",
    " * Hidden layer with 2 nodes\n",
    " * Output layer with 1 node\n",
    "\n",
    "The size of the dataset would be 33 rows with 2 features each (`years of experience` and `education level`) and 1 target value (`salary`).\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_13.png\" alt=\"Backpropagation implementation network\" width=\"485\" height=\"284\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 22.</b> Backpropagation - Simple neural network for our salary data</i></p>\n",
    "\n",
    "### Forward propagation\n",
    "$$Z^{(1)} = W^{(1)} X + B^{(1)}$$\n",
    "$$A^{(1)}1 = sigmoid(Z^{(1)})$$\n",
    "$$Z^{(2)} = W^{(2)} A^{(1)} + B^{(2)}$$\n",
    "$$A^{(2)} = sigmoid(Z^{(2)})$$\n",
    "\n",
    "\n",
    "### Backward propagation\n",
    "$$ Error = (A^{(2)} - Y) ^ 2$$\n",
    "$$ dA^{(2)} = 2 (A^{(2)} - Y) $$\n",
    "$$ dZ^{(2)} = sigmoid'(A^{(2)}) dA^{(2)} $$\n",
    "$$ dW^{(2)} = \\frac{1}{n} dZ^{(2)} \\cdot A^{(1)^T} $$\n",
    "$$ dB^{(2)} = \\frac{1}{n} \\sum dZ^{(2)} $$\n",
    "\n",
    "$$ dA^{(1)} = W^{(2)^T} \\cdot dZ^{(2)}  $$\n",
    "$$ dZ^{(1)} = sigmoid'(A^{(1)}) dA^{(1)} $$\n",
    "$$ dW^{(1)} = \\frac{1}{n} dZ^{(1)} \\cdot A^{(0)} $$\n",
    "\n",
    "### Variables and its shapes\n",
    "$$A^{(0)} = X : 33 \\times 2 $$\n",
    "$$Z^{(1)}, A^{(1)} : 2 \\times 33 $$\n",
    "$$Z^{(2)}, A^{(2)}, dA^{(2)}, dZ^{(2)}: 1 \\times 33 $$\n",
    "$$W^{(2)}, dW^{(2)}: 1 \\times 2 $$\n",
    "$$B^{(2)}, dB^{(2)}: 1 \\times 1 $$\n",
    "\n",
    "$$Z^{(1)}, A^{(1)}, dA^{(1)}, dZ^{(1)}: 2 \\times 33 $$\n",
    "$$W^{(1)}, dW^{(1)}: 2 \\times 2 $$\n",
    "$$B^{(1)}, dB^{(1)}: 2 \\times 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # Numpy is a library for numerical computing\n",
    "import matplotlib.pyplot as plt # Matplotlib is a library for creating static, animated, and interactive visualizations\n",
    "\n",
    "\n",
    "# TODO: delete me - Set the float formatter for numpy arrays\n",
    "float_formatter = \"{:.3f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X1 = years of experience\n",
    "X1 = [1.2, 1.3, 1.5, 1.8, 2, 2.1, 2.2, 2.5, 2.8, 2.9, 3.1, 3.3, 3.5, 3.8, 4, 4.1, 4.5, 4.9, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 10, 11, 12, 13, 14, 15]\n",
    "# X2 = level of education\n",
    "X2 = [2, 5, 3, 5, 3, 4, 2, 3, 4, 4, 3, 7, 5, 6, 5, 5, 2, 3, 4, 5, 6, 7, 5, 3, 2, 4, 5, 7, 3, 5, 7, 7, 5]\n",
    "# Y = salary\n",
    "Y = [2900, 3300, 3100, 4200, 3500, 3800, 3300, 3500, 3750, 4000, 3900, 5300, 4420, 5000, 4900, 5200, 3900, 4800, 5700, 6500, 6930, 7500, 7360, 6970, 6800, 7500, 8000, 9500, 11000, 9500, 12300, 13700, 12500]\n",
    "\n",
    "# Pandas data frame works with vectorized arrays (33 arrays of 1 element each)\n",
    "vectorized_X1 = np.array(X1).reshape(-1, 1)\n",
    "vectorized_X2 = np.array(X2).reshape(-1, 1)\n",
    "Y_train = np.array(Y).reshape(-1, 1) / 20000  # We divide by 20000 to scale down the output (it must be between 0 and 1)\n",
    "\n",
    "# Pack the train set\n",
    "X_train = np.concatenate([vectorized_X1, vectorized_X2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        # Initialize weights and biases using random numbers\n",
    "        self.weights = [np.random.randn(hidden_size, input_size), np.random.randn(output_size, hidden_size)]\n",
    "        self.biases = [np.zeros((hidden_size, 1)), np.zeros((output_size, 1))]\n",
    "\n",
    "    def sigmoid(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X: np.ndarray) -> np.ndarray:\n",
    "        return X * (1 - X)\n",
    "\n",
    "    def forward(self, X: np.ndarray, print_values: bool = False) -> list[np.ndarray]:\n",
    "        z1 = self.weights[0] @ X.T + self.biases[0]\n",
    "        a1 = self.sigmoid(z1)\n",
    "\n",
    "        z2 = self.weights[1] @ a1 + self.biases[1]\n",
    "        a2 = self.sigmoid(z2)\n",
    "\n",
    "        return [z1, z2], [a1, a2]\n",
    "\n",
    "    def backward(self, X: np.ndarray, Y: np.ndarray, a: list[np.ndarray]):\n",
    "        a1 = a[0]\n",
    "        a2 = a[1]\n",
    "\n",
    "        # Calculate the error\n",
    "        error = (a2 - Y.T) ** 2\n",
    "        n = len(Y)\n",
    "\n",
    "        dc_da2 = 2 * (a2 - Y.T)                             # Derivative of the error w.r.t. the activation\n",
    "        dc_dz2 = self.sigmoid_derivative(a2) * dc_da2       # Derivative of the error w.r.t. the delta of the output layer\n",
    "        dc_dw2 = np.dot(dc_dz2, a1.T) / n                   # Derivative of the error w.r.t. the weights of the output layer\n",
    "        dc_db2 = np.sum(dc_dz2, axis=1, keepdims=True) / n  # Derivative of the error w.r.t. the biases of the output layer\n",
    "\n",
    "        dc_da1 = self.weights[1].T @ dc_dz2                 # Derivative of the error w.r.t. the activation of the hidden layer\n",
    "        dc_dz1 = self.sigmoid_derivative(a1) * dc_da1       # Derivative of the error w.r.t. the delta of the hidden layer\n",
    "        dc_dw1 = np.dot(dc_dz1, X) / n                      # Derivative of the error w.r.t. the weights of the hidden layer\n",
    "        dc_db1 = np.sum(dc_dz1, axis=1, keepdims=True) / n  # Derivative of the error w.r.t. the biases of the hidden layer\n",
    "\n",
    "        return dc_dw1, dc_dw2, dc_db1, dc_db2, error\n",
    "\n",
    "    def update_weights_and_biases(self, dc_dw1: np.ndarray, dc_dw2: np.ndarray, dc_db1: np.ndarray, dc_db2: np.ndarray, learning_rate: float):\n",
    "        self.weights[0] -= learning_rate * dc_dw1\n",
    "        self.weights[1] -= learning_rate * dc_dw2\n",
    "        self.biases[0] -= learning_rate * dc_db1\n",
    "        self.biases[1] -= learning_rate * dc_db2\n",
    "    \n",
    "    def train(self, X: np.ndarray, Y: np.ndarray, learning_rate: float, epochs: int):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            z, a = self.forward(X)\n",
    "\n",
    "            dc_dw1, dc_dw2, dc_db1, dc_db2, error = self.backward(X, Y, a)\n",
    "            losses.append(np.sum(error))\n",
    "\n",
    "            # Update the weights and biases\n",
    "            self.update_weights_and_biases(dc_dw1, dc_dw2, dc_db1, dc_db2, learning_rate)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch} error: {round(np.sum(error), 3)}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 error: 6.222\n",
      "Epoch 1000 error: 0.168\n",
      "Epoch 2000 error: 0.034\n",
      "Epoch 3000 error: 0.023\n",
      "Epoch 4000 error: 0.021\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = NeuralNetwork(2, 2, 1)  # 2 input nodes, 2 hidden nodes, 1 output node\n",
    "losses = network.train(X_train, Y_train, 0.5, 5000)\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network with 2 neurons in the hidden layer and 1 neuron in the output layer was constructed. The network was trained for `5000` epochs with a learning rate of `0.5`. These parameters were found with a little trial and error.\n",
    "\n",
    "Running the example prints the accumulated prediction error. Obviously, the backpropagation works and minimze the error with each epoch going down to `0.021`, which is not bad at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
