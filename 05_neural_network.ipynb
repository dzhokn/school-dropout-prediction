{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For developing our **anomaly detection autoencoder** we would need to develop a neural network.\n",
    "\n",
    "## What is Neural Network?\n",
    "\n",
    "A neural network is a simplified computational model inspired by the way the human brain processes information. While not biologically precise, it mimics key learning mechanisms observed in neural systems. The brain consists of a vast network of neurons connected by synapses, which vary in **strength**. Stronger connections, formed through **repeated activation**, facilitate faster and more efficient signal transmission. For instance, touching a hot pan triggers a learned response via a well-established neural pathway that prompts immediate withdrawal. Similarly, neural networks in ML strengthen certain connections - represented by weights - through repeated exposure and learning, **favoring pathways that lead to more accurate predictions**.\n",
    "\n",
    "This is a highly simplified explanation, but hopefully it helps you understand the basic concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How does Neural Network work?\n",
    "Now let’s understand how a Neural Network is represented. A neural network consists of many **Nodes** (Neurons) grouped in **Layers**. Each layer can have any number of nodes and a neural network can have any number of layers. Let’s have a closer look at a couple of layers.\n",
    "\n",
    "<center><img src=\"img/neural_network_1.png\" alt=\"Neural Network Basic Layers\" width=\"346\" height=\"419\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 4.</b> Layers in a Neural Network</i></p>\n",
    "\n",
    "Now as you can see, there are many interconnections between both the layers. These interconnections exist between **each node** in the first layer with **each and every node** in the second layer. These are also called the **weights** between two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weights\n",
    "Now let’s see how exactly these **weights** operate.\n",
    "<center><img src=\"img/neural_network_2.png\" alt=\"Neural Network Basic Layers\" width=\"500\" height=\"393\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 5.</b> How single neuron operates</i></p>\n",
    "\n",
    "$$ H = x_1*w_1 + x_2*w_2 + x_3*w_3 + b $$\n",
    "$$ Y = f(H) $$\n",
    "\n",
    "Here we take the example of what’s going on with a **single node** in the network. Here we are considering all the values from the **previous layer** connecting to **one node in the next layer**.\n",
    "\n",
    "* $Y$ is the **final output value** of the node.\n",
    "* $w_i$ are the **weights** between the nodes in the previous layer and the output node.\n",
    "* $x_i$ are the **values of the nodes** of the previous layer.\n",
    "* $b$ is a **constant** bias. Bias is essentially a weight without an input term. It’s useful for having an **extra bit of adjustability** which is not dependant on previous layer.\n",
    "* $H$ is the *intermediate node value*. This is not the final value of the node.\n",
    "* $f( )$ is called an **Activation Function** and it is something we can choose. We will go through it’s importance later.\n",
    "\n",
    "So finally, the output value of this node will be $f(0.57)$\n",
    "\n",
    "Now let’s look at the calculations between two complete layers:\n",
    "\n",
    "<center><img src=\"img/neural_network_3.png\" alt=\"Calculations between Neural Network layers\" width=\"500\" height=\"583\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 6.</b> Calculations between two layers</i></p>\n",
    "\n",
    "The weights in this case have been colour coded for easier understanding. We can represent the entire calculation as a matrix multiplication. If we represent the weights corresponding to each input node as vectors and arrange them horizontally, we can form a matrix, this is called the weight matrix. Now we can multiply the **weight matrix** with the input vector and then add the bias vector to get the intermediate node values.\n",
    "\n",
    "<center><img src=\"img/neural_network_4.png\" alt=\"Formula for calculating output values of a layer\" width=\"500\" height=\"481\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 7.</b> Formula for calculating output values of a layer</i></p>\n",
    "\n",
    "We can summarize the entire calculation as $Y = f(w*x + b)$. Here, $Y$ is the output vector, $x$ is the input vector, $w$ represents the weight matrix between the two layers and $b$ is the bias vector.\n",
    "\n",
    "We can determine the size of the weight matrix by looking at the number of input nodes and output nodes. An $m*n$ weight matrix means that it is between two layers with the first layer having $n$ nodes and the second layer having $m$ nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "Now let’s look at a complete neural network.\n",
    "\n",
    "\n",
    "<center><img src=\"img/neural_network_5.png\" alt=\"Formula for calculating the whole neural network\" width=\"500\" height=\"542\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 8.</b> Formula for calculating the whole neural network</i></p>\n",
    "\n",
    "This is a small neural network of four layers. The input layer is where we feed our **external stimulus**, or basically the **data** from which our neural network has to **learn from**. The output layer is where we are supposed to get the target value, this represents what exactly our neural network is trying to **predict** or **learn**. All layers in between are called **hidden layers**. When we feed the inputs into the first layer, the values of the nodes will be calculated layer by layer using the matrix multiplications and activation functions till we get the final values at the output layer. That is how we get an output from a neural network.\n",
    "\n",
    "So essentially a neural network is, simply put, a series of matrix multiplications and activation functions. When we input a vector containing the input data, the data is multiplied with the sequence of weight matrices and subjected to activation functions untill it reaches the output layer, which contains the **predictions** of the neural network corresponding to that particular input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now let's implement a basic neural network. For the sake of the example we will be generating **random weights** for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        # Initialize weights and biases\n",
    "        # NB: Remember, the weight matrix size is m x n, where m is the number of output nodes and n is the number of input nodes\n",
    "        weight_matrix_1 = np.random.randn(hidden_size, input_size) # Weight matrix between input and hidden layer\n",
    "        weight_matrix_2 = np.random.randn(output_size, hidden_size) # Weight matrix between hidden and output layer\n",
    "\n",
    "        self.weights = [weight_matrix_1, weight_matrix_2]\n",
    "        self.biases = [np.zeros(hidden_size), np.zeros(output_size)]\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray: # Activation function (there are many other activation functions)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> list[np.ndarray]:\n",
    "        activations = [x]           # List to store activations for each layer. Keep the first input X too.\n",
    "        current_activation = x      # Current activation value (the first time it is simply the input X)\n",
    "        # Propagate through each layer\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, current_activation) + b   # Apply the layer's weights and bias\n",
    "            current_activation = self.sigmoid(z)    # Apply activation function and get the layer's output\n",
    "            activations.append(current_activation)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke the neural network (remember, we use random weights) and see how our input data of three numbers $(1,2,3)$ is converted into a final output after passing through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74349774 0.24478919]\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(3, 4, 2)  # 3 input nodes, 4 hidden nodes, 2 output nodes\n",
    "input_data = np.array([1, 2, 3])\n",
    "final_layer = network.forward(input_data)[-1]  # Get final layer output\n",
    "\n",
    "print(final_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
